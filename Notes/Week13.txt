In v2.6, there were a new struct called struct fdtable introduced into files_struct:

    struct files_struct {
        atomic_t count;
        struct fdtable __rcu *fdt;
        struct fdtable fdtab;

        spinlock_t file_lock __cacheline_aligned_in_smp;
        int next_fd;
        struct embedded_fd_set close_on_exec_init;
        struct embedded_fd_set open_fds_init;
        struct file __rcu *fd_array[NR_OPEN_DEFAULT];
    };

    struct fdtable {
        unsigned int max_fds;
        struct file __rcu **fd; //current fd array
        fd_set *close_on_exec;
        fd_set *open_fds;
        struct rcu_head rcu;
        struct fdtable *next;
    };

struct fdtable was used to tidy up the structure a bit more. With this version, these structs were also carried into a new file, which
is "include/linux/fdtable.h".

The way pointers point to each other are as follows:

files_struct has the pointer fdt -> it points to fdtab -> fdtab contains fd pointer -> fd array has file descriptor informations

The kernel always starts from fdt pointer. At the beginning, fdt pointer points to the fdtab struct instance. In fdtab instance, as it
was in the older versions, fd, close_on_exec, open_fds pointers exist. These pointers, at the beginning, point to their counterparts
(fd_array, close_on_exec_init and open_fds_init respectively) in files_struct. However, as things progress, fdt pointer inside
files_struct can point to another fdtable instance, and pointers inside the fdtable can also point to other, bigger instances.

So how do we access a file that is pointed by the file descriptor using current macro?
current->fdt->fd[fdx]

There were also macros and functions introduced to simplify this access. These functions also make some sanity checks:

    static inline struct file * fcheck_files(struct files_struct *files, unsigned int fd);
    #define fcheck(fd) fcheck_files(current->files, fd)

There is also a higher level function called fget that directly gets the file from the file descriptor in v2.6:

    struct file * fget(unsigned int fd)
    {
        return __fget(fd, FMODE_PATH);
    }
    EXPORT_SYMBOL(fget);

fget also does some synchronization things in case of there being other updaters. It traverses through the pointer structure mentioned
above while doing the synchronization and other necessary checks and returns the file from the file descriptor table. fget is also
exported, meaning it is a function that can be used by drivers as well.

In modern kernels (v6), fd_set isn't used anymore. The bit arrays are directly created by a long array instead of fd_set.
current->files->fdt->fd[fdx] can also be used in these versions. Doing this access in a safe way are also implemented via kernel
functions. fget function is still available and can be used the same way.

As mentioned in the previous weeks, the main problem is finding the very first bit that is zero in order to find the first available
file descriptor. In modern kernels, this is done by using two level bitmaps. full_fds_bits represents the first level bitma and
opend_fds represents the second level bitmap. Whether or not all file descriptors are full or not is represented in open_fds.
full_fds_bits bitmap then represents the index of the first element in the open_fds where not all bits of the long data is 0. So how
is this actually done in modern kernels?

There is a function called find_next_zero_bit in the kernel that has been in use for a long time now. As mentioned before, internally
it uses CPU specific assembly instructions under the hood. These instructions are written with gcc compiler extensions that allow 
writing assembly instructions in C code.

The traversal of functions after calling sys_open is as follows:
sys_open --> do_sys_open --> do_sys_openat2 --> __get_unused_fd_flags --> alloc_fd --> find_next_fd --> find_next_zero_bit

The implementation of find_next_fd in modern kernels is as follows (please note that the first and second level mentioned in the
kernel comments is the reverse of the level that is mentioned in the course notes. meaning, the course notes say the first used level
is the first and then the second level gets used. In kernel comments and codes however, the first used level is the second level and
the secondly used level is the first level. In kernel codes, the first level bitmap represents whether the file descriptors are full
or not, and the second level bitmap represents the index of the first zero bit element in the first level. Kernel's first level is
open_fds and kernel's second level is full_fds_bits):

    static unsigned int find_next_fd(struct fdtable *fdt, unsigned int start)
    {
        unsigned int maxfd = fdt->max_fds; //always multiple of BITS_PER_LONG
        unsigned int maxbit = maxfd / BITS_PER_LONG;
        unsigned int bitbit = start / BITS_PER_LONG;
        unsigned int bit;

        //Try to avoid looking at the second level bitmap
        bit = find_next_zero_bit(&fdt->open_fds[bitbit], BITS_PER_LONG, start & (BITS_PER_LONG - 1));

        if (bit < BITS_PER_LONG)
            return bit + bitbit * BITS_PER_LONG;

        bitbit = find_next_zero_bit(fdt->full_fds_bits, maxbit, bitbit) * BITS_PER_LONG;
        if (bitbit >= maxfd)
            return maxfd;
        if (bitbit > start)
            start = bitbit;
        return find_next_zero_bit(fdt->open_fds, maxfd, start);
    }

The first parameter of the function takes the address of the fdtable instance, second parameter typically takes the next_fds, which
represents the descriptor number that the search will start from.

As a summary:
1) The kernel doesn't immediately go to second level bitmap (full_fds_bits). It first searches on a single unsigned long element of
the open_fds array.
2) If the search is unsuccessful, the index of the first 0 bit in the second level bitmap is obtained. In the first level (open_fds),
only the unsigned long element in this index is searched.
3) find_next_zero_bit function uses small_const_nbits function internally to decide whether the search will include only a single
element of the unsigned long array or all the elements after that element.

Following the codes further will show that the search is done by ffz and __ffs instructions at the assembly level.

So far, we talked about:
-Data structures in the task_struct about the open files
-How empty descriptors are stored
-How the lowest empty descriptor is obtained

Now, we move on to the data structures of the file systems and talk about how the kernel handles the file operations.

In POSIX systems (and therefore in Linux), threads doesn't have separate file descriptor tables. File descriptors and file descriptor
tables are process specific. So when a process creates a thread, the file information is specific to the process and its the threads
can access this file information. When copying the data into the newly created thread, a shallow copy operation is done (aka only the
pointers are copied, not the underlying data). This is one of the most important reasons why so many pointers are used in the
implementation of the structs mentioned above and in the previous week.

This means that in the end, there is only a single files_struct instance per process.

When fork() is called, child process can also see the same open files with the parent process. How is this implemented in modern
kernels?

Let the open file's file descriptor be 3. When the process forks, a completely identical process will be created. After fork the child
process will be also able to use the file descriptor 3. Parent process' file descriptor with ID 3 points to the same file instance as
the child process' file descriptor with ID 3. However, for the files that are opened AFTER the fork operation, they must be different.
So, a new file struct instance is created. A new file descriptor table is created. That part is done with deep copying. But the
intialization of the deep copied table is done using the parent's file descriptor table. Basically, their past is the same but their
future needs to be different and the setup is done accordingly.

Now let's move on to the kernel's data structures and fundamental file operation functions.
The kernel holds the information about the open files in a struct called "struct file". Some of the elements of this struct can be
enabled/disabled with kernel compilation options. Here are some important members of this struct:

-File pointer's position in the member "loff_t f_pos"
-Various flags about the file is stored in the member "unsigned int f_flags" (such as O_APPEND or O_RDWR used when opening the file,
file's rwx access rights on the disc, type of the file etc.)
-User and group ID informations of the file in "unsigned int f_uid, f_gid"
-File's directory information in "struct dentry *f_dentry" (dentry stands for directory entry). Later moved into struct path structure
-The mode of the file when opened with the open function in "mode_t f_mode".
-How many file descriptors point to the file in "atomic_t f_count"
-File's disc information in struct inode *f_inode

Note: Some of these members can be a bit different in terms of types and locations. The tutor showed us these from different versions
of the kernel and since I can't copy-paste the way he did I tried my best to note them as well as I can. Some version might've gotten
mixed up.

There can be multiple references to a given file, which is held in the f_count variable as mentioned before. As the file is closed by
these references, the f_count is decreased and once it reaches zero, the file object is deleted.

So how does the kernel access the file on the disc while working on an open file? There are required metadata about the files that are
stored on the disc (such as last read time, last modified time, file length etc.). In ext file systems, these informations are held
in a place called the "i-node block". When the file is being opened, the kernel reads these informations from the disc and puts them
in a struct called "struct inode". This way, the kernel doesn't have to go to the disc every single time it needs an information that
is stored in there.

So what about the directory information of the file? When a new file is opened, the kernel reads its directory data (such as file 
path and some other information) into another struct called "struct dentry".

To summarize:
-struct file: Struct used to make operation on open files
-struct inode: Struct used to hold meta data information about the file on the disk
-struct dentry: Struct used to hold the location of the file in the file system and some other related information

So how can we reach the inode and dentry structs? Up until and including v2.6, the address of the dentry struct was held under the
file struct. dentry struct then held the address of the inode struct. So the chain was as shown below:

file struct --> dentry struct --> inode struct

Later down the line, inode struct's address was also put into file struct for ease of access. Obviously, inode can still be accessed
from the dentry struct. Otherwise it would be a catastrophy for backwards compatibility.

In UNIX/Linux systems, a file system can be mounted somewhere in the root directory. Meaning, different file paths that can be
accessed from the same file system can exist in different physical devices. This also creates the situation where the kernel needs to
be able to understand what file system a mounted path is in (think using ext4 on Linux but then mounting Windows's NTFS that exist on
the other SSD in the PC). The information required to achieve this will be called the mount information in the course. Before v2.4,
the mount information was held in struct dentry. Starting with v2.4, "struct vfsmount" was introduced and put into struct file for
better organization. After v2.6, an open file's struct dentry address and vfsmount information was put into yet another struct called
"struct path", which is stored in struct file as the element f_path.

Imagine the case where the same file was opened multiple times via the "open" function.

    fd1 = open("test.txt", O_RDONLY);
    fd2 = open("test.txt", O_RDONLY);

When this is done, independent of whether or not the file is the same file, the kernel creates a new struct file instance for each
open call. Because each time we open up a file, their file descriptors will be different. File descriptors are stored struct file in
instances. However, since this is the same file on the disk, there is no need to create new struct inode and struct dentry instances.
Moreover, different processes can also be accessing the same file on the disk; for which there is still no need to create new struct
inode and struct dentry instances. Therefore the kernel doesn't create new instances for these structs when the same file is opened
multiple times.

This also means that when a new file is opened, the kernel needs to check if this file already has inode and dentry structs instances.
For this, the kernel uses the cache mentioned above. There are two different caches; "dentry cache" and "inode cache". When a new file
is opened, if the correesponding dentry and inode instances exist in these caches, they are directly used without getting the disc
involved at all. Even when all the processes close an open file, the dentry and inode information stick around in this cache. This is
because some files are frequently opened and closed, such as library files, or /etc/passwd file. Having them in the cache instead of
going to the disc each time they are needed improves performance quite a lot. Of course, these cache systems have a size limit. When
they are full, some of the dentry and inode instances are released from the cache. In Linux, these cache systems are generally use an
algorithm called LRU (Least Recently Used). Meaning when instances from the cache need to be released, the released ones aren't the
ones that are used the least overall, but only the ones that haven't been used in a while. This is because if instances really need
to be kept around, it will mostly likely be constantly and frequently in use.

Now let's talk about the relationship between dentry cache, inode cache and page cache.
Page cache is organized based on disc blocks. It is used to transfer every type of disc block and is a low level cache.
Dentry and inode caches on the other hand, are caches that only handle dentry and inode struct instances respectively. When a new file
is opened, if the dentry and inode instances doesn't exist in the cache, they are obtained from the disc. Before the disc reading is
done, page cache is checked to see if the required blocks exist in the page cache.

To summarize, what happens once a file is opened is as follows:
1) An empty file descriptor from the process' file descriptor table is found quickly.
2) A struct file instance is created and initialized.
3) dentry and inode struct instances are created if they don't already exist in their respective caches.
4) The address of the struct file instance is put into the file descriptor table (fd array) and the index at which it is placed is
returned as the file descriptor.

This is what we have done so far. However, there are also other things that need to happen. One of these things are called the
"pathname resolution".

The root directory and the current working directory of a given process is held in the process control block (task_struct instance) as
talked about in the first few weeks. In modern kernels, this information is held as follows:

    struct task_struct {
        //other members...
        struct fs_struct *fs;
        //other members...
    };

    struct fs_struct {
        //other members...
        struct path root, pwd;
        //other members...
    } __randomize_layout;;

    struct path {
        struct vfsmount *mnt;
        struct dentry *dentry;
    } __randomize_layout;

As it can be seen, the kernel holds the root and working directories of a given process as dentry structs.